{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucasaaz/Implementa-o-de-Assistente-Conversacional-Baseado-em-LLM/blob/main/AS05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comando Git"
      ],
      "metadata": {
        "id": "Yb-tAMbRw2C0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instale o Git (caso não esteja instalado)\n",
        "!apt-get install git -y\n",
        "\n",
        "# Configure seu usuário (substitua com seus dados)\n",
        "!git config --global user.name \"Lucas\"\n",
        "!git config --global user.email \"lucas.augustoaz@gmail.com\""
      ],
      "metadata": {
        "id": "CMBOoNqduZ5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitua pela URL do SEU repositório criado no GitHub\n",
        "!git clone https://github.com/lucasaaz/Implementa-o-de-Assistente-Conversacional-Baseado-em-LLM.git\n",
        "\n",
        "# Acesse a pasta do repositório\n",
        "%cd nome-do-repositorio"
      ],
      "metadata": {
        "id": "ShN-acx5usTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copie o notebook atual para o repositório\n",
        "!cp /content/Seu_Notebook.ipynb /content/Implementa-o-de-Assistente-Conversacional-Baseado-em-LLM/\n",
        "\n",
        "# Copie outros arquivos necessários (requirements.txt, etc)\n",
        "!cp /content/requirements.txt /Implementa-o-de-Assistente-Conversacional-Baseado-em-LLM/"
      ],
      "metadata": {
        "id": "HkCwflQuu3Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adicione todos os arquivos\n",
        "!git add .\n",
        "\n",
        "# Commit das alterações\n",
        "!git commit -m \"Versão inicial do projeto - Publicação do Colab\"\n",
        "\n",
        "# Faça o push para o GitHub (insira seu token quando pedido)\n",
        "# Gere um token em: GitHub > Settings > Developer Settings > Personal Access Tokens\n",
        "!git push https://github.com/lucasaaz/Implementa-o-de-Assistente-Conversacional-Baseado-em-LLM.git main"
      ],
      "metadata": {
        "id": "nXX7tYXCu_Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementação de Assistente Conversacional Baseado em LLM**"
      ],
      "metadata": {
        "id": "Ps0R5BCDwdoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "# # Assistente Conversacional Baseado em LLM - AS05\n",
        "\n",
        "# %%\n",
        "# Instalação de dependências\n",
        "!pip install -q langchain==0.1.4 openai==1.12.0 faiss-cpu==1.7.4 pypdf==3.17.4 tiktoken==0.5.2 python-dotenv==1.0.0 gradio==4.44.1 --upgrade\n",
        "!pip install -q pdf2image pytesseract pillow\n",
        "!sudo apt install -y tesseract-ocr\n",
        "!sudo apt install -y libtesseract-dev\n",
        "\n",
        "# %%\n",
        "import os\n",
        "import logging\n",
        "import tempfile\n",
        "from dotenv import load_dotenv\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.docstore.document import Document\n",
        "import gradio as gr\n",
        "\n",
        "# Configuração de logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# %%\n",
        "# Configuração\n",
        "load_dotenv()\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") or input(\"Digite sua OpenAI API Key: \")\n",
        "\n",
        "# %%\n",
        "def super_safe_string(text, default=\"\"):\n",
        "    \"\"\"Versão ultra-protegida para conversão de strings\"\"\"\n",
        "    try:\n",
        "        if text is None:\n",
        "            return default\n",
        "        if isinstance(text, str):\n",
        "            return text\n",
        "        return str(text) if text else default\n",
        "    except:\n",
        "        return default\n",
        "\n",
        "class PDFAssistant:\n",
        "    def __init__(self):\n",
        "        self.vectorstore = None\n",
        "        self.qa_chain = None\n",
        "        self.embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "        self.llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=OPENAI_API_KEY)\n",
        "        logger.info(\"Assistente inicializado com sucesso\")\n",
        "\n",
        "    def extract_text_with_fallback(self, pdf_path):\n",
        "        \"\"\"Extrai texto com múltiplos fallbacks\"\"\"\n",
        "        try:\n",
        "            # Tenta extração normal primeiro\n",
        "            loader = PyPDFLoader(pdf_path)\n",
        "            docs = loader.load()\n",
        "            if docs and super_safe_string(docs[0].page_content):\n",
        "                return docs\n",
        "\n",
        "            # Fallback para OCR se necessário\n",
        "            logger.info(f\"Tentando OCR para {pdf_path}\")\n",
        "            images = convert_from_path(pdf_path)\n",
        "            ocr_docs = []\n",
        "            for i, img in enumerate(images):\n",
        "                text = pytesseract.image_to_string(img)\n",
        "                if text.strip():\n",
        "                    ocr_docs.append(Document(\n",
        "                        page_content=text,\n",
        "                        metadata={\"source\": pdf_path, \"page\": i+1}\n",
        "                    ))\n",
        "            return ocr_docs if ocr_docs else [Document(page_content=\"\", metadata={\"source\": pdf_path})]\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro na extração: {str(e)}\")\n",
        "            return [Document(page_content=\"\", metadata={\"source\": pdf_path})]\n",
        "\n",
        "    def load_and_process_pdfs(self, pdf_files):\n",
        "        \"\"\"Processamento totalmente seguro de PDFs\"\"\"\n",
        "        if not pdf_files:\n",
        "            return \"⚠️ Nenhum arquivo PDF foi enviado\"\n",
        "\n",
        "        documents = []\n",
        "        for pdf_file in pdf_files:\n",
        "            try:\n",
        "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
        "                    tmp_path = tmp_file.name\n",
        "                    tmp_file.write(pdf_file.read())\n",
        "\n",
        "                docs = self.extract_text_with_fallback(tmp_path)\n",
        "                valid_docs = [doc for doc in docs if super_safe_string(doc.page_content).strip()]\n",
        "                documents.extend(valid_docs)\n",
        "                os.remove(tmp_path)\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Erro processando {pdf_file.name}: {str(e)}\")\n",
        "                if os.path.exists(tmp_path):\n",
        "                    os.remove(tmp_path)\n",
        "                continue\n",
        "\n",
        "        if not documents:\n",
        "            return \"⚠️ Não foi possível extrair texto válido dos PDFs\"\n",
        "\n",
        "        try:\n",
        "            text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=1000,\n",
        "                chunk_overlap=200,\n",
        "                length_function=len\n",
        "            )\n",
        "            chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "            self.vectorstore = FAISS.from_documents(chunks, self.embeddings)\n",
        "            self.qa_chain = RetrievalQA.from_chain_type(\n",
        "                self.llm,\n",
        "                retriever=self.vectorstore.as_retriever(\n",
        "                    search_type=\"mmr\",\n",
        "                    search_kwargs={\"k\": 4}\n",
        "                ),\n",
        "                return_source_documents=True\n",
        "            )\n",
        "            return f\"✅ {len(chunks)} chunks processados com sucesso\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro no processamento final: {str(e)}\")\n",
        "            return f\"❌ Erro no processamento: {str(e)}\"\n",
        "\n",
        "    def ask_question(self, question):\n",
        "        \"\"\"Método ultra-protegido para perguntas\"\"\"\n",
        "        try:\n",
        "            safe_question = super_safe_string(question).strip()\n",
        "            if not safe_question:\n",
        "                return \"⚠️ Pergunta inválida ou vazia\"\n",
        "\n",
        "            if not self.qa_chain:\n",
        "                return \"ℹ️ Documentos não carregados. Envie PDFs primeiro.\"\n",
        "\n",
        "            result = self.qa_chain({\"query\": safe_question}) or {}\n",
        "\n",
        "            answer = super_safe_string(result.get(\"result\"), \"Resposta não disponível\")\n",
        "\n",
        "            sources = []\n",
        "            for doc in result.get(\"source_documents\", []):\n",
        "                try:\n",
        "                    source = super_safe_string(doc.metadata.get(\"source\"), \"Fonte desconhecida\")\n",
        "                    page = super_safe_string(doc.metadata.get(\"page\"))\n",
        "                    if page:\n",
        "                        source += f\" (página {page})\"\n",
        "                    sources.append(source)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            seen = set()\n",
        "            unique_sources = [x for x in sources if not (x in seen or seen.add(x))]\n",
        "\n",
        "            return f\"Resposta: {answer}\\n\\nFontes:\\n\" + \"\\n\".join(f\"- {src}\" for src in unique_sources)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"ERRO CRÍTICO: {str(e)}\", exc_info=True)\n",
        "            return \"❌ Erro interno. Tente novamente ou recarregue os documentos.\"\n",
        "\n",
        "# %%\n",
        "assistant = PDFAssistant()\n",
        "\n",
        "# %%\n",
        "css = \"\"\"\n",
        "footer {visibility: hidden}\n",
        ".gr-box {border: 1px solid #e2e2e2; border-radius: 8px;}\n",
        ".gr-button-primary {background: #4f46e5; color: white;}\n",
        "\"\"\"\n",
        "\n",
        "def create_response(question, chat_history):\n",
        "    try:\n",
        "        response = assistant.ask_question(question)\n",
        "        return \"\", chat_history + [(question, response)]\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Erro na interface: {str(e)}\")\n",
        "        return \"\", chat_history + [(question, \"❌ Erro no sistema. Tente novamente.\")]\n",
        "\n",
        "with gr.Blocks(title=\"Assistente de PDF\", theme=\"soft\", css=css) as demo:\n",
        "    gr.Markdown(\"\"\"# 📄 Assistente de PDF Inteligente\"\"\")\n",
        "\n",
        "    with gr.Tab(\"📤 Carregar PDFs\"):\n",
        "        gr.Markdown(\"Envie seus documentos para análise\")\n",
        "        files = gr.File(file_types=[\".pdf\"], file_count=\"multiple\")\n",
        "        upload_btn = gr.Button(\"Processar\", variant=\"primary\")\n",
        "        status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "    with gr.Tab(\"💬 Conversar\"):\n",
        "        chatbot = gr.Chatbot(height=400)\n",
        "        msg = gr.Textbox(placeholder=\"Digite sua pergunta...\")\n",
        "        send_btn = gr.Button(\"Enviar\", variant=\"primary\")\n",
        "        clear_btn = gr.Button(\"Limpar\")\n",
        "\n",
        "    upload_btn.click(\n",
        "        fn=lambda x: assistant.load_and_process_pdfs(x),\n",
        "        inputs=files,\n",
        "        outputs=status\n",
        "    )\n",
        "\n",
        "    send_btn.click(\n",
        "        fn=create_response,\n",
        "        inputs=[msg, chatbot],\n",
        "        outputs=[msg, chatbot]\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        fn=lambda: [],\n",
        "        inputs=[],\n",
        "        outputs=chatbot\n",
        "    )\n",
        "\n",
        "# %%\n",
        "try:\n",
        "    demo.launch(\n",
        "        share=True,\n",
        "        debug=True,\n",
        "        server_name=\"0.0.0.0\"\n",
        "    )\n",
        "except Exception as e:\n",
        "    logger.error(f\"Falha ao iniciar: {str(e)}\")\n",
        "    raise e"
      ],
      "metadata": {
        "id": "Ny6hBHWwjWPh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}