{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Comando Git"
      ],
      "metadata": {
        "id": "Yb-tAMbRw2C0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instale o Git (caso n√£o esteja instalado)\n",
        "!apt-get install git -y\n",
        "\n",
        "# Configure seu usu√°rio (substitua com seus dados)\n",
        "!git config --global user.name \"Lucas\"\n",
        "!git config --global user.email \"lucas.augustoaz@gmail.com\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMBOoNqduZ5S",
        "outputId": "872327d7-2d28-4818-b441-3d68d10fca26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.12).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitua pela URL do SEU reposit√≥rio criado no GitHub\n",
        "!git clone https://github.com/lucasaaz/Implementa-o-de-Assistente-Conversacional-Baseado-em-LLM.git\n",
        "\n",
        "# Acesse a pasta do reposit√≥rio\n",
        "%cd nome-do-repositorio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShN-acx5usTv",
        "outputId": "87b8ed91-18ec-471a-bcbc-037acce43691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Implementa-o-de-Assistente-Conversacional-Baseado-em-LLM'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (3/3), done.\n",
            "[Errno 2] No such file or directory: 'nome-do-repositorio'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copie o notebook atual para o reposit√≥rio\n",
        "!cp /content/Seu_Notebook.ipynb /content/Implementa-o-de-Assistente-Conversacional-Baseado-em-LLM/\n",
        "\n",
        "# Copie outros arquivos necess√°rios (requirements.txt, etc)\n",
        "!cp /content/requirements.txt /Implementa-o-de-Assistente-Conversacional-Baseado-em-LLM/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkCwflQuu3Rk",
        "outputId": "803e89fd-551f-4922-f1d1-12ee292a7bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/Seu_Notebook.ipynb': No such file or directory\n",
            "cp: cannot create regular file '/Implementa-o-de-Assistente-Conversacional-Baseado-em-LLM/': Not a directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adicione todos os arquivos\n",
        "!git add .\n",
        "\n",
        "# Commit das altera√ß√µes\n",
        "!git commit -m \"Vers√£o inicial do projeto - Publica√ß√£o do Colab\"\n",
        "\n",
        "# Fa√ßa o push para o GitHub (insira seu token quando pedido)\n",
        "# Gere um token em: GitHub > Settings > Developer Settings > Personal Access Tokens\n",
        "!git push https://github.com/lucasaaz/Implementa-o-de-Assistente-Conversacional-Baseado-em-LLM.git main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXX7tYXCu_Gf",
        "outputId": "65a25cf5-fdd0-43b3-f2e3-36e18fb42a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementa√ß√£o de Assistente Conversacional Baseado em LLM**"
      ],
      "metadata": {
        "id": "Ps0R5BCDwdoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "# # Assistente Conversacional Baseado em LLM - AS05\n",
        "\n",
        "# %%\n",
        "# Instala√ß√£o de depend√™ncias\n",
        "!pip install -q langchain==0.1.4 openai==1.12.0 faiss-cpu==1.7.4 pypdf==3.17.4 tiktoken==0.5.2 python-dotenv==1.0.0 gradio==4.44.1 --upgrade\n",
        "!pip install -q pdf2image pytesseract pillow\n",
        "!sudo apt install -y tesseract-ocr\n",
        "!sudo apt install -y libtesseract-dev\n",
        "\n",
        "# %%\n",
        "import os\n",
        "import logging\n",
        "import tempfile\n",
        "from dotenv import load_dotenv\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.docstore.document import Document\n",
        "import gradio as gr\n",
        "\n",
        "# Configura√ß√£o de logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# %%\n",
        "# Configura√ß√£o\n",
        "load_dotenv()\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") or input(\"Digite sua OpenAI API Key: \")\n",
        "\n",
        "# %%\n",
        "def super_safe_string(text, default=\"\"):\n",
        "    \"\"\"Vers√£o ultra-protegida para convers√£o de strings\"\"\"\n",
        "    try:\n",
        "        if text is None:\n",
        "            return default\n",
        "        if isinstance(text, str):\n",
        "            return text\n",
        "        return str(text) if text else default\n",
        "    except:\n",
        "        return default\n",
        "\n",
        "class PDFAssistant:\n",
        "    def __init__(self):\n",
        "        self.vectorstore = None\n",
        "        self.qa_chain = None\n",
        "        self.embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "        self.llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=OPENAI_API_KEY)\n",
        "        logger.info(\"Assistente inicializado com sucesso\")\n",
        "\n",
        "    def extract_text_with_fallback(self, pdf_path):\n",
        "        \"\"\"Extrai texto com m√∫ltiplos fallbacks\"\"\"\n",
        "        try:\n",
        "            # Tenta extra√ß√£o normal primeiro\n",
        "            loader = PyPDFLoader(pdf_path)\n",
        "            docs = loader.load()\n",
        "            if docs and super_safe_string(docs[0].page_content):\n",
        "                return docs\n",
        "\n",
        "            # Fallback para OCR se necess√°rio\n",
        "            logger.info(f\"Tentando OCR para {pdf_path}\")\n",
        "            images = convert_from_path(pdf_path)\n",
        "            ocr_docs = []\n",
        "            for i, img in enumerate(images):\n",
        "                text = pytesseract.image_to_string(img)\n",
        "                if text.strip():\n",
        "                    ocr_docs.append(Document(\n",
        "                        page_content=text,\n",
        "                        metadata={\"source\": pdf_path, \"page\": i+1}\n",
        "                    ))\n",
        "            return ocr_docs if ocr_docs else [Document(page_content=\"\", metadata={\"source\": pdf_path})]\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro na extra√ß√£o: {str(e)}\")\n",
        "            return [Document(page_content=\"\", metadata={\"source\": pdf_path})]\n",
        "\n",
        "    def load_and_process_pdfs(self, pdf_files):\n",
        "        \"\"\"Processamento totalmente seguro de PDFs\"\"\"\n",
        "        if not pdf_files:\n",
        "            return \"‚ö†Ô∏è Nenhum arquivo PDF foi enviado\"\n",
        "\n",
        "        documents = []\n",
        "        for pdf_file in pdf_files:\n",
        "            try:\n",
        "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
        "                    tmp_path = tmp_file.name\n",
        "                    tmp_file.write(pdf_file.read())\n",
        "\n",
        "                docs = self.extract_text_with_fallback(tmp_path)\n",
        "                valid_docs = [doc for doc in docs if super_safe_string(doc.page_content).strip()]\n",
        "                documents.extend(valid_docs)\n",
        "                os.remove(tmp_path)\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Erro processando {pdf_file.name}: {str(e)}\")\n",
        "                if os.path.exists(tmp_path):\n",
        "                    os.remove(tmp_path)\n",
        "                continue\n",
        "\n",
        "        if not documents:\n",
        "            return \"‚ö†Ô∏è N√£o foi poss√≠vel extrair texto v√°lido dos PDFs\"\n",
        "\n",
        "        try:\n",
        "            text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=1000,\n",
        "                chunk_overlap=200,\n",
        "                length_function=len\n",
        "            )\n",
        "            chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "            self.vectorstore = FAISS.from_documents(chunks, self.embeddings)\n",
        "            self.qa_chain = RetrievalQA.from_chain_type(\n",
        "                self.llm,\n",
        "                retriever=self.vectorstore.as_retriever(\n",
        "                    search_type=\"mmr\",\n",
        "                    search_kwargs={\"k\": 4}\n",
        "                ),\n",
        "                return_source_documents=True\n",
        "            )\n",
        "            return f\"‚úÖ {len(chunks)} chunks processados com sucesso\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro no processamento final: {str(e)}\")\n",
        "            return f\"‚ùå Erro no processamento: {str(e)}\"\n",
        "\n",
        "    def ask_question(self, question):\n",
        "        \"\"\"M√©todo ultra-protegido para perguntas\"\"\"\n",
        "        try:\n",
        "            safe_question = super_safe_string(question).strip()\n",
        "            if not safe_question:\n",
        "                return \"‚ö†Ô∏è Pergunta inv√°lida ou vazia\"\n",
        "\n",
        "            if not self.qa_chain:\n",
        "                return \"‚ÑπÔ∏è Documentos n√£o carregados. Envie PDFs primeiro.\"\n",
        "\n",
        "            result = self.qa_chain({\"query\": safe_question}) or {}\n",
        "\n",
        "            answer = super_safe_string(result.get(\"result\"), \"Resposta n√£o dispon√≠vel\")\n",
        "\n",
        "            sources = []\n",
        "            for doc in result.get(\"source_documents\", []):\n",
        "                try:\n",
        "                    source = super_safe_string(doc.metadata.get(\"source\"), \"Fonte desconhecida\")\n",
        "                    page = super_safe_string(doc.metadata.get(\"page\"))\n",
        "                    if page:\n",
        "                        source += f\" (p√°gina {page})\"\n",
        "                    sources.append(source)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            seen = set()\n",
        "            unique_sources = [x for x in sources if not (x in seen or seen.add(x))]\n",
        "\n",
        "            return f\"Resposta: {answer}\\n\\nFontes:\\n\" + \"\\n\".join(f\"- {src}\" for src in unique_sources)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"ERRO CR√çTICO: {str(e)}\", exc_info=True)\n",
        "            return \"‚ùå Erro interno. Tente novamente ou recarregue os documentos.\"\n",
        "\n",
        "# %%\n",
        "assistant = PDFAssistant()\n",
        "\n",
        "# %%\n",
        "css = \"\"\"\n",
        "footer {visibility: hidden}\n",
        ".gr-box {border: 1px solid #e2e2e2; border-radius: 8px;}\n",
        ".gr-button-primary {background: #4f46e5; color: white;}\n",
        "\"\"\"\n",
        "\n",
        "def create_response(question, chat_history):\n",
        "    try:\n",
        "        response = assistant.ask_question(question)\n",
        "        return \"\", chat_history + [(question, response)]\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Erro na interface: {str(e)}\")\n",
        "        return \"\", chat_history + [(question, \"‚ùå Erro no sistema. Tente novamente.\")]\n",
        "\n",
        "with gr.Blocks(title=\"Assistente de PDF\", theme=\"soft\", css=css) as demo:\n",
        "    gr.Markdown(\"\"\"# üìÑ Assistente de PDF Inteligente\"\"\")\n",
        "\n",
        "    with gr.Tab(\"üì§ Carregar PDFs\"):\n",
        "        gr.Markdown(\"Envie seus documentos para an√°lise\")\n",
        "        files = gr.File(file_types=[\".pdf\"], file_count=\"multiple\")\n",
        "        upload_btn = gr.Button(\"Processar\", variant=\"primary\")\n",
        "        status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "    with gr.Tab(\"üí¨ Conversar\"):\n",
        "        chatbot = gr.Chatbot(height=400)\n",
        "        msg = gr.Textbox(placeholder=\"Digite sua pergunta...\")\n",
        "        send_btn = gr.Button(\"Enviar\", variant=\"primary\")\n",
        "        clear_btn = gr.Button(\"Limpar\")\n",
        "\n",
        "    upload_btn.click(\n",
        "        fn=lambda x: assistant.load_and_process_pdfs(x),\n",
        "        inputs=files,\n",
        "        outputs=status\n",
        "    )\n",
        "\n",
        "    send_btn.click(\n",
        "        fn=create_response,\n",
        "        inputs=[msg, chatbot],\n",
        "        outputs=[msg, chatbot]\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        fn=lambda: [],\n",
        "        inputs=[],\n",
        "        outputs=chatbot\n",
        "    )\n",
        "\n",
        "# %%\n",
        "try:\n",
        "    demo.launch(\n",
        "        share=True,\n",
        "        debug=True,\n",
        "        server_name=\"0.0.0.0\"\n",
        "    )\n",
        "except Exception as e:\n",
        "    logger.error(f\"Falha ao iniciar: {str(e)}\")\n",
        "    raise e"
      ],
      "metadata": {
        "id": "Ny6hBHWwjWPh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}